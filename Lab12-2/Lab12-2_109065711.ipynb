{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d174321b",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e20efcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# You'll generate plots of attention in order to see which parts of an image\n",
    "# our model focuses on during captioning\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn includes many helpful utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86c3057",
   "metadata": {},
   "source": [
    "# Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0247eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = './words_captcha/'\n",
    "annotation_file = './words_captcha/spec_train_val.txt'\n",
    "CHAR_NUM=26\n",
    "TRAIN_NUM=100000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802f9d08",
   "metadata": {},
   "source": [
    "# Read annotation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "99d0cc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(annotation_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "img_name = []\n",
    "annotation_list = []\n",
    "test_name = []\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip('\\n')\n",
    "    line = line.split(' ')\n",
    "    img_name.append(IMAGE_DIR+line[0]+'.png')\n",
    "    annotation_list.append(line[1])\n",
    "for i in range(120000, 140000):\n",
    "    test_name.append('./words_captcha/a' + str(i)+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33470b9a",
   "metadata": {},
   "source": [
    "# Train Val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "33fd412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first 30,000 captions from the shuffled set\n",
    "\n",
    "train_name=img_name[:TRAIN_NUM]\n",
    "val_name=img_name[TRAIN_NUM:]\n",
    "train_annotation=annotation_list[:TRAIN_NUM]\n",
    "val_annotation=annotation_list[TRAIN_NUM:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1259f58c",
   "metadata": {},
   "source": [
    "# Start code: + End code: -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "476127ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_to_idx = {}\n",
    "idx_to_character = {}\n",
    "character_to_idx[' '] = 0\n",
    "idx_to_character[0] = ' '\n",
    "index = 1\n",
    "\n",
    "for i in range(CHAR_NUM):\n",
    "    character_to_idx[chr(ord('a') + i)] = index\n",
    "    idx_to_character[index] = chr(ord('a') + i)\n",
    "    index += 1\n",
    "    \n",
    "character_to_idx['+'] = 27\n",
    "idx_to_character[27] = '+'\n",
    "character_to_idx['-'] = 28\n",
    "idx_to_character[28] = '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b5524b3a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " 'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " '+': 27,\n",
       " '-': 28}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "14eecfde",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ' ',\n",
       " 1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 27: '+',\n",
       " 28: '-'}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "08825df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def max_length(annotations):\n",
    "    max_len = 0\n",
    "    for annotation in annotations:\n",
    "        if len(annotation) > max_len:\n",
    "            max_len = len(annotation)\n",
    "    return max_len\n",
    "max_len = max_length(annotation_list) + 2\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40452d17",
   "metadata": {},
   "source": [
    "# Index word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bc79e768",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotation_idx = []\n",
    "val_annotation_idx = []\n",
    "\n",
    "for annotation in train_annotation:\n",
    "    annotation_idx = [27]\n",
    "    for char in annotation:\n",
    "        annotation_idx.append(character_to_idx[char])\n",
    "    annotation_idx.append(28)\n",
    "    while len(annotation_idx) < max_len:\n",
    "        annotation_idx.append(0)\n",
    "    train_annotation_idx.append(annotation_idx)\n",
    "    \n",
    "for annotation in val_annotation:\n",
    "    annotation_idx = [27]\n",
    "    for char in annotation:\n",
    "        annotation_idx.append(character_to_idx[char])\n",
    "    annotation_idx.append(28)\n",
    "    while len(annotation_idx) < max_len:\n",
    "        annotation_idx.append(0)\n",
    "    val_annotation_idx.append(annotation_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2c10aac6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[27, 20, 8, 21, 19, 28, 0],\n",
       " [27, 23, 23, 23, 28, 0, 0],\n",
       " [27, 20, 9, 5, 4, 28, 0],\n",
       " [27, 9, 4, 19, 28, 0, 0],\n",
       " [27, 10, 1, 13, 28, 0, 0]]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_annotation_idx[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d08f38e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./words_captcha/a0.png',\n",
       " './words_captcha/a1.png',\n",
       " './words_captcha/a2.png',\n",
       " './words_captcha/a3.png',\n",
       " './words_captcha/a4.png']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_name[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c4102",
   "metadata": {},
   "source": [
    "# Para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a95df97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 40\n",
    "BUFFER_SIZE = 5000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = len(char2idx)\n",
    "train_steps = len(train_name) // BATCH_SIZE\n",
    "val_steps = len(val_name) // BATCH_SIZE\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8b048cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(image_path, annotation):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (300, 160))\n",
    "    img = img/255 - 1.\n",
    "    return img, annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc712b18",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fae86f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_name,train_annotation_idx))\n",
    "train_dataset = train_dataset.map(load, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_name,val_annotation_idx))\n",
    "val_dataset = val_dataset.map(load, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = val_dataset.shuffle(BUFFER_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6c9a0d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (300, 160))\n",
    "    img = img/255 - 1.\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "932bea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_name)\n",
    "test_dataset = test_dataset.map(load_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "963c5abb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 300, 160, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 7), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56c613d",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "598ed0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, 64, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519c6973",
   "metadata": {},
   "source": [
    "# Use CNN similar to VGG16 to be Feature_Extracter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f407b551",
   "metadata": {},
   "source": [
    "Using the inceptionV3 without fine tune would have bad results, accuracy=0.0005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc57ca3",
   "metadata": {},
   "source": [
    "Because of using VGG16 takes long time to train, I remove some layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b81d98ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_relu(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, size, stride):\n",
    "        super(conv_relu, self).__init__()\n",
    "        self.conv = tf.keras.layers.Conv2D(filters, size, stride, padding=\"same\",\n",
    "                      kernel_initializer=tf.keras.initializers.TruncatedNormal())\n",
    "        self.batchnorm = tf.keras.layers.BatchNormalization()\n",
    "        self.lkrelu = tf.keras.layers.LeakyReLU(0.1)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        x = self.conv(inputs)\n",
    "        x = self.batchnorm(x,training = training)\n",
    "        x = self.lkrelu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8989742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Extracter(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Feature_Extracter, self).__init__()\n",
    "        self.cr1 = conv_relu(64,3,1)\n",
    "        self.cr2 = conv_relu(64,3,1)\n",
    "        self.max_pooling1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        self.cr3 = conv_relu(128,3,1)\n",
    "        self.cr4 = conv_relu(128,3,1)\n",
    "        self.max_pooling2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        self.cr5 = conv_relu(256,3,1)\n",
    "        self.cr6 = conv_relu(256,3,1)\n",
    "        self.cr7 = conv_relu(256,3,1)\n",
    "        self.max_pooling3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        self.cr8 = conv_relu(512,3,1)\n",
    "        self.cr9 = conv_relu(512,3,1)\n",
    "        self.max_pooling4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        self.cr11 = conv_relu(512,3,1)\n",
    "        self.cr12 = conv_relu(512,3,1)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        x = self.cr1(inputs,training)\n",
    "        x = self.cr2(x,training)\n",
    "        x = self.max_pooling1(x)\n",
    "        x = self.cr3(x,training)\n",
    "        x = self.cr4(x,training)\n",
    "        x = self.max_pooling2(x)\n",
    "        x = self.cr5(x,training)\n",
    "        x = self.cr6(x,training)\n",
    "        x = self.cr7(x,training)\n",
    "        x = self.max_pooling3(x)\n",
    "        x = self.cr8(x,training)\n",
    "        x = self.cr9(x,training)\n",
    "        x = self.max_pooling4(x)\n",
    "        x = self.cr11(x,training)\n",
    "        x = self.cr12(x,training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e2e6ce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c1312bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"feature__extracter_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv_relu_34 (conv_relu)    multiple                  2048      \n",
      "                                                                 \n",
      " conv_relu_35 (conv_relu)    multiple                  37184     \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  multiple                 0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv_relu_36 (conv_relu)    multiple                  74368     \n",
      "                                                                 \n",
      " conv_relu_37 (conv_relu)    multiple                  148096    \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  multiple                 0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv_relu_38 (conv_relu)    multiple                  296192    \n",
      "                                                                 \n",
      " conv_relu_39 (conv_relu)    multiple                  591104    \n",
      "                                                                 \n",
      " conv_relu_40 (conv_relu)    multiple                  591104    \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPoolin  multiple                 0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv_relu_41 (conv_relu)    multiple                  1182208   \n",
      "                                                                 \n",
      " conv_relu_42 (conv_relu)    multiple                  2361856   \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPoolin  multiple                 0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv_relu_43 (conv_relu)    multiple                  2361856   \n",
      "                                                                 \n",
      " conv_relu_44 (conv_relu)    multiple                  2361856   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,007,872\n",
      "Trainable params: 10,001,472\n",
      "Non-trainable params: 6,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feature_extracter = Feature_Extracter()\n",
    "feature_extracter.build((None,300,160,3))\n",
    "feature_extracter.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1b9cc4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f692406b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2ead8b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ec22f0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e1a38072",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b28cf7",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a5b2ffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f906e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([character_to_idx['+']] * BATCH_SIZE, 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = feature_extracter(img_tensor,True)\n",
    "        features = tf.reshape(features,(features.shape[0], -1, features.shape[3]))\n",
    "        features = encoder(features)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = feature_extracter.trainable_variables + encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6ee4a2b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [26:07<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train Loss 0.022931\n",
      "Time taken for 1 epoch 1568.3571164608002 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [26:23<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Train Loss 0.019200\n",
      "Time taken for 1 epoch 1584.1744413375854 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [25:58<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Train Loss 0.013007\n",
      "Time taken for 1 epoch 1558.6700956821442 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [25:55<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Train Loss 0.011771\n",
      "Time taken for 1 epoch 1556.1789474487305 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [25:54<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Train Loss 0.009851\n",
      "Time taken for 1 epoch 1554.8824553489685 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 79/2500 [00:55<28:27,  1.42it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11768\\1265449833.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mbatch_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mt_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch {} Train Loss {:.6f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2494\u001b[0m       (graph_function,\n\u001b[0;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2497\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1860\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1861\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1863\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    #total_val_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in tqdm(enumerate(train_dataset), total=train_steps):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "    print ('Epoch {} Train Loss {:.6f}'.format(epoch + 1, total_loss/train_steps))\n",
    "    loss_plot.append(total_loss / train_steps)\n",
    "\n",
    "    ckpt_manager.save()\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805467f3",
   "metadata": {},
   "source": [
    "# Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "daa7a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(img_tensor, target):\n",
    "    val_loss = 0\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "    dec_input = tf.expand_dims([char2idx['']]*BATCH_SIZE, 1)\n",
    "    features = feature_extracter(img_tensor,False)\n",
    "    features = tf.reshape(features,(features.shape[0], -1, features.shape[3]))\n",
    "    features = encoder(features)\n",
    "    result = np.full((BATCH_SIZE, 1), 27)\n",
    "    for i in range(1, target.shape[1]):\n",
    "        # passing the features through the decoder\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "        predicted_id = tf.argmax(predictions,axis=1).numpy()\n",
    "        val_loss += loss_function(target[:, i], predictions)\n",
    "        result = np.concatenate((result, predicted_id.reshape((BATCH_SIZE,1))), axis=1)\n",
    "        dec_input = tf.expand_dims(predicted_id, 1)\n",
    "    \n",
    "    return val_loss, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7d8dc3d5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:24<00:00,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy 0.975700, Validation Loss 0.021711\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "equal_num = 0\n",
    "total_val_loss = 0\n",
    "for (batch, (img_tensor, target)) in tqdm(enumerate(val_dataset), total=val_steps):\n",
    "#     val_loss = 0\n",
    "#     hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "#     dec_input = tf.expand_dims([char2idx['']]*BATCH_SIZE, 1)\n",
    "#     features = feature_extracter(img_tensor,False)\n",
    "#     features = tf.reshape(features,(features.shape[0], -1, features.shape[3]))\n",
    "#     features = encoder(features)\n",
    "#     result = np.full((BATCH_SIZE, 1), 27)\n",
    "#     for i in range(1, target.shape[1]):\n",
    "#         # passing the features through the decoder\n",
    "#         predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "#         predicted_id = tf.argmax(predictions,axis=1).numpy()\n",
    "#         val_loss += loss_function(target[:, i], predictions)\n",
    "#         result = np.concatenate((result, predicted_id.reshape((BATCH_SIZE,1))), axis=1)\n",
    "#         dec_input = tf.expand_dims(predicted_id, 1)\n",
    "    val_loss, result = val_step(img_tensor, target)\n",
    "    target_array = target.numpy()\n",
    "    total_val_loss += (val_loss / int(target.shape[1]))\n",
    "    for i in range(BATCH_SIZE):\n",
    "        for j in range(max_len):\n",
    "            if result[i][j] == 28 and target_array[i][j] == 28:\n",
    "                if (result[i][1:j] == target_array[i][1:j]).all():\n",
    "                    equal_num+=1\n",
    "                break\n",
    "print ('Validation Accuracy {:.6f}, Validation Loss {:.6f}'.format(float(equal_num)/(20000.),total_val_loss/val_steps),end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38027805",
   "metadata": {},
   "source": [
    "Accuracy is higher than 0.9, job is done!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b3f48f",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8887bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH='./Lab12-2_109065711.txt'\n",
    "START_INDEX=120000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "80d9d711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_txt(FILE_PATH, START_INDEX):\n",
    "    num=0\n",
    "    for batch, img_tensor in tqdm(enumerate(test_dataset), total = len(test_name)//BATCH_SIZE):\n",
    "        hidden = decoder.reset_state(batch_size=BATCH_SIZE)\n",
    "        dec_input = tf.expand_dims([char2idx['']]*BATCH_SIZE, 1)\n",
    "        features = feature_extracter(img_tensor,False)\n",
    "        features = tf.reshape(features,(features.shape[0], -1, features.shape[3]))\n",
    "        features = encoder(features)\n",
    "        result = np.full((BATCH_SIZE, 1), 27)\n",
    "        for i in range(1, max_len):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "            predicted_id = tf.argmax(predictions,axis=1).numpy()\n",
    "            result = np.concatenate((result, predicted_id.reshape((BATCH_SIZE,1))), axis=1)\n",
    "            dec_input = tf.expand_dims(predicted_id, 1)\n",
    "        for i in range(BATCH_SIZE):\n",
    "            output_str = ''\n",
    "            for j in range(1,max_len):\n",
    "                if result[i][j] == 28:\n",
    "                    break\n",
    "                else:\n",
    "                    output_str = output_str + idx2char[result[i][j]]\n",
    "            with open(FILE_PATH,'a') as f:\n",
    "                f.write('a' + str(START_INDEX + num) + ' ' + output_str+'\\n')\n",
    "            f.close()\n",
    "            num += 1\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9dc364ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:15<00:00,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_txt(FILE_PATH, START_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a1b698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
