{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wu13sperZAG"
   },
   "source": [
    "#組員\n",
    "朱彥如 109065711\n",
    "張誠晉 110062609\n",
    "蘇東誠 108061123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTHpabr_s6QC"
   },
   "source": [
    "#Summary\n",
    "##Feature Extraction \n",
    "*  Channels\n",
    "*  Time\n",
    "*  Weekend\n",
    "*  number of Images\n",
    "*  released day(Sun, Mon, ...) \n",
    "*  number of links\n",
    "*  number of videos\n",
    "*  content\n",
    "*  Word2vec(not used)\n",
    "*  number of youtube links\n",
    "*  time (hours/month/date/hour)\n",
    "*  number of tag a\n",
    "*  distance to 5 LDAs (tile)\n",
    "*  distance to 5 LDAs (content)\n",
    "*  keywords to tfidf\n",
    "*  contens to tfidf\n",
    "*  top 10 channels\n",
    "*  top 50 catagories\n",
    "*  ave of content non-zero tfidfs\n",
    "*  number of non-zero tfidfs\n",
    "*  author names\n",
    "\n",
    "## Extarction method\n",
    "use regular expression or other parsing tools\n",
    "\n",
    "## Tried Classifiers\n",
    "* SVC\n",
    "* Random Forest\n",
    "* Category Bayes\n",
    "* SGD(log loss, hinge loss)\n",
    "* Adaboost\n",
    "* Xgboost\n",
    "\n",
    "## Conclusion\n",
    "1. Random Forest performs better than other models in this case, and it has lower requirment for the training data (e.g. no normalization is ok)\n",
    "2. Randome Forest can perform better with higher `n_estimator` value. However, the improvement shrinks and the traing time increases as we increase `n_estimator`. Thus, we chose a number between 1000 to 1400.\n",
    "\n",
    "3. `np.get_dummy()` is a convenient function to transform any feature, like authors, to one-hot feature\n",
    "\n",
    "4. we can apply one dummy(one-hot) result to another by \n",
    "`dummy2 = dummy2.reindex(columns = dummy1.columns, fill_value=0)`\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqP5V666lMSZ"
   },
   "source": [
    "# Import things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9A6t3oT_Hv4Z",
    "outputId": "b3f20193-7184-4a43-ad94-1ee57626a3df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8426b840"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "from pylab import *\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\"\"\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nn29IH-ar_J0"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dxN2KVKulMSc"
   },
   "outputs": [],
   "source": [
    "## YR update\n",
    "def preprocessor(o_text):\n",
    "    r='(>[^<]*[Mm]ashable[^<]*<)|(>[^<]*MASHABLE[^<]*<)'\n",
    "    o_text = re.sub(r, '><', o_text)\n",
    "    # remove content in time tag\n",
    "    text = BeautifulSoup(o_text)\n",
    "    time_tag = text.find(\"time\")\n",
    "    # remove a\n",
    "    a_tag = text.find_all(\"a\")\n",
    "    # remove em\n",
    "    em_tag = text.find_all(\"em\")\n",
    "    # <div class=\"credit\">\n",
    "    credit_tag = text.find_all(\"div\" ,class_='credit') \n",
    "    # image-credit\n",
    "    image_credit_tag = text.find_all(\"div\" ,class_='image-credit')\n",
    "    # <div class=\"guest-author-summary\">\n",
    "    geust_auth_tag = text.find_all(\"div\" ,class_='guest-author-summary')\n",
    "   \n",
    "    credit_tag=set(credit_tag)\n",
    "    for i in credit_tag:\n",
    "        o_text = o_text.replace(str(i),'')\n",
    "        \n",
    "    em_tag=set(em_tag)\n",
    "    for i in em_tag:\n",
    "        o_text = o_text.replace(str(i),'')\n",
    "        \n",
    "    geust_auth_tag=set(geust_auth_tag)\n",
    "    for i in geust_auth_tag:\n",
    "        o_text = o_text.replace(str(i),'')\n",
    "\n",
    "    image_credit_tag=set(image_credit_tag)\n",
    "    for i in image_credit_tag:\n",
    "        o_text = o_text.replace(str(i),'')\n",
    "    \n",
    "    a_tag=set(a_tag)\n",
    "    for i in a_tag:\n",
    "        o_text = o_text.replace(str(i),'')\n",
    "    \n",
    "    text = o_text.replace(str(time_tag),'')\n",
    "\n",
    "    # remove HTML tags\n",
    "    sp = BeautifulSoup(text)\n",
    "    text=sp.find('section',class_='article-content').get_text()\n",
    "    #text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "     \n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    \n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VxKcAAiolMSd"
   },
   "outputs": [],
   "source": [
    "def df_to_LDAMatrix(input_df):\n",
    "    #_new_counts = count_vect.transform(input_df['Page content_text'])\n",
    "    def_dtm = LDA_count_df.transform(input_df['Page content_text'])\n",
    "    tr_M=LDA.transform(def_dtm)\n",
    "    #tr_M = hstack([tr_M, input_df['Weekend'].values.reshape(-1, 1), input_df['Fri'].values.reshape(-1, 1), input_df['img'].values.reshape(-1, 1), input_df['pop ch'].values.reshape(-1, 1)])\n",
    "    concatenate_M=np.concatenate([tr_M, input_df['Page content_text'].values.reshape(-1, 1), input_df['Fri'].values.reshape(-1, 1), input_df['img'].values.reshape(-1, 1), input_df['pop ch'].values.reshape(-1, 1)],axis=1)\n",
    "    return concatenate_M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5DF_vrElMSd"
   },
   "source": [
    "# PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30a00a3a",
    "outputId": "4eb57d37-252b-4de0-e1fa-806598378ff1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5db69653"
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,1),\n",
    "                        preprocessor=preprocessor,\n",
    "                        tokenizer=tokenizer_stem_nostop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2f286bf"
   },
   "outputs": [],
   "source": [
    "tfidf = pickle.load(open(\"/content/drive/MyDrive/大四上資料/Deep_learning /COMP1/vectorizer.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9295a444"
   },
   "outputs": [],
   "source": [
    "test_tfidf = tfidf.transform(test['Page content'])\n",
    "test_tfidf = hstack([test_tfidf, test['Weekend'].values.reshape(-1, 1), test['Image'].values.reshape(-1, 1)])\n",
    "pred = clf.predict_proba(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03468570"
   },
   "outputs": [],
   "source": [
    "pred1 = [round(x[1],1) for x in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6cc37d95"
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['Id'] = submit_sample['Id']\n",
    "submission['Popularity'] = pred1\n",
    "print(submission)\n",
    "submission.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eW_5hRJ5hsG_"
   },
   "source": [
    "#Feature Selection 0\n",
    "## \"\"in the title\n",
    "by 東誠\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZUFH_MHoh65J",
    "outputId": "f17ac22b-58fa-4ee2-a72b-feb58bb28726"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Popularity                                  Page content_text  Weekend  \\\n",
      "0   0          -1   clara moskowitz for space com nasa s grand ch...        0   \n",
      "1   1           1  by christina warrengoogle s new open source pa...        0   \n",
      "2   2           1  by sam lairdballin 2014 nfl draft picks get to...        0   \n",
      "3   3          -1  by sam lairdcameraperson fails deliver slapsti...        0   \n",
      "4   4          -1  by connor finnegannfl star helps young fan pro...        0   \n",
      "\n",
      "        img                                              title      link  \\\n",
      "0  0.008929  NASA's Grand Challenge: Stop Asteroids From De...  0.070513   \n",
      "1  0.017857  Google's New Open Source Patent Pledge: We Won...  0.057692   \n",
      "2  0.017857  Ballin': 2014 NFL Draft Picks Get to Choose Th...  0.035256   \n",
      "3  0.008929        Cameraperson Fails Deliver Slapstick Laughs  0.041667   \n",
      "4  0.464286  NFL Star Helps Young Fan Prove Friendship With...  0.051282   \n",
      "\n",
      "     iframe                                       Page content    figure  Fri  \\\n",
      "0  0.000000  <html><head><div class=\"article-info\"> <span c...  0.009009  0.0   \n",
      "1  0.000000  <html><head><div class=\"article-info\"><span cl...  0.009009  0.0   \n",
      "2  0.274725  <html><head><div class=\"article-info\"><span cl...  0.234234  0.0   \n",
      "3  0.230769  <html><head><div class=\"article-info\"><span cl...  0.189189  1.0   \n",
      "4  0.000000  <html><head><div class=\"article-info\"><span cl...  0.459459  0.0   \n",
      "\n",
      "   pop ch  world  socme  app  marketing            author  \n",
      "0       0    0.0    0.0  0.0        0.0               NaN  \n",
      "1       0    1.0    0.0  0.0        0.0  Christina Warren  \n",
      "2       0    1.0    0.0  0.0        0.0         Sam Laird  \n",
      "3       0    1.0    0.0  0.0        0.0         Sam Laird  \n",
      "4       0    1.0    0.0  0.0        0.0   Connor Finnegan  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/content/drive/MyDrive/大四上資料/Deep_learning /COMP1/preprocess_df.csv')\n",
    "print(df[\"title\"].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQ4zZZ9jxTOg"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "#print(df[\"title\"].head(5))\n",
    "quot_num = []\n",
    "for title in df[\"title\"].values:\n",
    "  #print(len(re.findall(\"\\'\\w+\\'\", title)))\n",
    "  quot_num.append(len(re.findall(\"\\'\\w+\\'\", title)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dt2poKAEkPAk",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Feature Selection 1\n",
    "by 誠晉\n",
    "\n",
    "*  Time(hours)\n",
    "*  Weekend\n",
    "*  number of Images\n",
    "*  every kind of channels\n",
    "*  released day(1-31)\n",
    "*  released time in one-hot('0'-'24')\n",
    "*  week days in one-hot('Sun'-'Sat')\n",
    "*  number of links\n",
    "*  number of videos\n",
    "*  LDA distances\n",
    "*  released month(1-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mFdZGQPajB2L",
    "outputId": "883ee167-e396-4e80-cc41-fdee98febdfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'Popularity', 'Page content', 'Time', 'Weekend', 'Img', 'World',\n",
      "       'Tech', 'Entertain', 'Business', 'Watercooler', 'Startups', 'Music',\n",
      "       'Gadgets', 'Social-media', 'Apps', 'Marketing', 'Media', 'Gaming',\n",
      "       'Mobile', 'Us', 'Pics', 'Dev', 'Small-busine', 'Film', 'How-to', 'Day',\n",
      "       '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n",
      "       '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24',\n",
      "       'Sun', 'Wed', 'Fri', 'Tue', 'Mon', 'Thu', 'Sat', 'Link', 'Video',\n",
      "       'LDA1', 'LDA2', 'LDA3', 'LDA4', 'LDA5', 'Monthnum'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('df.csv') # input dataframe\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extarction method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time\n",
    "time = [str(BeautifulSoup(x).find(\"time\"))[33:35] for x in df['Page content']]\n",
    "df['Time'] = [int(x) for x in time]\n",
    "for x in range(25):\n",
    "    df[str(x)] = [1 if y == x else 0 for y in df['Time']]\n",
    "    \n",
    "# Image\n",
    "df['Image'] = [len(BeautifulSoup(x).find_all(\"img\")) for x in df['Page content']]\n",
    "\n",
    "# Article\n",
    "Article = [str(BeautifulSoup(x).find(\"article\"))[22:35] for x in df['Page content']]\n",
    "for x in {'world','tech','entertain','business','watercooler','startups','music','gadgets','social-media','apps','marketing','gaming','mobile','us','pics','dev','small-busine','film','how-to'}:\n",
    "    df[x] = [1 if y.startswith(x) else 0 for y in Article]\n",
    "\n",
    "# Day\n",
    "df['Day'] = [int(str(BeautifulSoup(x).find(\"time\"))[21:23]) for x in df['Page content']]    \n",
    "    \n",
    "# Weekdays\n",
    "Weekdays = [str(BeautifulSoup(x).find(\"time\"))[16:19] for x in df['Page content']]\n",
    "df['Weekend'] = [1 if (x == 'Sat' or x == 'Sun') else 0 for x in Weekdays]\n",
    "for x in {\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"}:\n",
    "    df[x] = [1 if y == x else 0 for y in Weekdays]\n",
    "    \n",
    "# Video\n",
    "df['Video'] = [len(BeautifulSoup(x).find_all(\"iframe\")) for x in df['Page content']]\n",
    "\n",
    "# Link\n",
    "df['Link'] = [len(BeautifulSoup(x).find_all(\"a\")) for x in df['Page content']]\n",
    "\n",
    "# LDA\n",
    "text =tfidf.transform(df['Page content'])\n",
    "lda = LatentDirichletAllocation(n_components=5)\n",
    "lda.fit(text)\n",
    "LDA = lda.transform(text)\n",
    "df['LDA1'] = [x[0] for x in LDA]\n",
    "df['LDA2'] = [x[1] for x in LDA]\n",
    "df['LDA3'] = [x[2] for x in LDA]\n",
    "df['LDA4'] = [x[3] for x in LDA]\n",
    "df['LDA5'] = [x[4] for x in LDA]\n",
    "\n",
    "# Month\n",
    "def M(x):\n",
    "    if x == 'Jan':\n",
    "        return 1\n",
    "    if x == 'Feb':\n",
    "        return 2\n",
    "    if x == 'Mar':\n",
    "        return 3\n",
    "    if x == 'Apr':\n",
    "        return 4\n",
    "    if x == 'May':\n",
    "        return 5\n",
    "    if x == 'Jun':\n",
    "        return 6\n",
    "    if x == 'Jul':\n",
    "        return 7\n",
    "    if x == 'Aug':\n",
    "        return 8\n",
    "    if x == 'Sep':\n",
    "        return 9\n",
    "    if x == 'Oct':\n",
    "        return 10\n",
    "    if x == 'Nov':\n",
    "        return 11\n",
    "    if x == 'Dec':\n",
    "        return 12\n",
    "month = [str(BeautifulSoup(x).find(\"time\"))[24:27] for x in df['Page content']]\n",
    "df['Monthnum'] = [M(x) for x in month]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NU2lzLqRmVLE"
   },
   "source": [
    "# Feature Selection 2\n",
    "by 彥如\n",
    "\n",
    "*  num of images\n",
    "*  number of youtube links\n",
    "*  time (hours/month/date/hour)\n",
    "*  Mon Tues, Wed, ...\n",
    "*  is weekend? \n",
    "*  number of tag a\n",
    "*  distance to 5 LDAs (tile)\n",
    "*  distance to 5 LDAs (content)\n",
    "*  keywords to tfidf\n",
    "*  contens to tfidf\n",
    "*  top 10 channels\n",
    "*  top 50 catagories\n",
    "*  ave of content non-zero tfidfs\n",
    "*  number of non-zero tfidfs\n",
    "*  author names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TDRCLC2lMSk"
   },
   "source": [
    "#### Extarction method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNhISDGwlMSk"
   },
   "outputs": [],
   "source": [
    "def get_time(content):\n",
    "    sp=BeautifulSoup(content)\n",
    "    tmp_sp=sp.find_all('time')\n",
    "    if(len(tmp_sp)==0):\n",
    "        return \"\"\n",
    "    else:\n",
    "        try:\n",
    "            return tmp_sp[0]['datetime']\n",
    "        except KeyError:\n",
    "            return \"\"\n",
    "def time_df(input_df):\n",
    "    input_df['time']=input_df['Page content'].apply(get_time)\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1y4L2vilMSk"
   },
   "outputs": [],
   "source": [
    "def is_Weekend(content):\n",
    "    sp=BeautifulSoup(content)\n",
    "    tmp_sp=sp.find_all('time')\n",
    "    if(len(tmp_sp)==0):\n",
    "        return 0\n",
    "    else:\n",
    "        try:\n",
    "            is_Sun=tmp_sp[0]['datetime'].find('Sun')\n",
    "            is_Sat=tmp_sp[0]['datetime'].find('Sat')\n",
    "            if (is_Sun==-1 and is_Sat==-1):\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "        except KeyError:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SKEbf_HlMSl"
   },
   "outputs": [],
   "source": [
    "def catch_title(content):\n",
    "    sp=BeautifulSoup(content)\n",
    "    tmp_sp=sp.find_all('h1')\n",
    "    title=tmp_sp[0].get_text()\n",
    "    return title\n",
    "def df_cathc_title(tmp_df):\n",
    "    tmp_df['title']=tmp_df['Page content'].apply(catch_title)\n",
    "    return tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tk2_qi2KlMSl"
   },
   "outputs": [],
   "source": [
    "def img_count(content):\n",
    "    sp=BeautifulSoup(content)\n",
    "    tmp_sp=sp.find_all('img')\n",
    "    return len(tmp_sp)\n",
    "def img_df(input_df):\n",
    "    input_df['img']=input_df['Page content'].apply(img_count)\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K7Sbf8t4lMSl"
   },
   "outputs": [],
   "source": [
    "def ytv_count(content):\n",
    "    sp=BeautifulSoup(content)\n",
    "    tmp_sp=sp.find_all('iframe')\n",
    "    count=0\n",
    "    for i in tmp_sp:\n",
    "        try:\n",
    "            ind=i['src'].find('youtube')\n",
    "            if (ind>-1):\n",
    "                count=count+1\n",
    "        except:\n",
    "            continue\n",
    "    return count\n",
    "\n",
    "def ytv_df(df_tmp):\n",
    "    df_tmp['youtube vedio']=df_tmp['Page content'].apply(ytv_count)\n",
    "    return df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdWmDE1PlMSm"
   },
   "outputs": [],
   "source": [
    "def figure_count(content):\n",
    "    sp=BeautifulSoup(content)\n",
    "    tmp_sp=sp.find_all('figure')\n",
    "    return len(tmp_sp)\n",
    "\n",
    "def figure_df(df_tmp):\n",
    "    df_tmp['figure']=df_tmp['Page content'].apply(figure_count)\n",
    "    return df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WyQnu0BzlMSm"
   },
   "outputs": [],
   "source": [
    "def link_count(content):\n",
    "    sp=BeautifulSoup(content)\n",
    "    tmp_sp=sp.find_all('a')\n",
    "    return len(tmp_sp)\n",
    "\n",
    "def link_df(df_tmp):\n",
    "    df_tmp['link']=df_tmp['Page content'].apply(link_count)\n",
    "    return df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1o6eeMvblMSn"
   },
   "outputs": [],
   "source": [
    "def get_channel(content):\n",
    "    sp=BeautifulSoup(content)\n",
    "    try:\n",
    "        channel=sp.find('article')['data-channel']\n",
    "        return channel          \n",
    "    except:\n",
    "        return \"\"\n",
    "def channel_df(df_tmp):\n",
    "    df_tmp['channel']=df_tmp['Page content'].apply(get_channel)\n",
    "    return df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8L80F3LlMSo"
   },
   "outputs": [],
   "source": [
    "def get_pop_ch(content):\n",
    "    sp=BeautifulSoup(content)\n",
    "    try:\n",
    "        channel=sp.find('article')['data-channel']\n",
    "        if (channel==\"socmed\" or channel==\"sports\" or channel==\"comics\" or channel==\"memes\" or channel==\"viral\" or channel==\"mob\" or channel==\"howto\"):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3W1dVrflMSo"
   },
   "outputs": [],
   "source": [
    "def get_author(content):\n",
    "    sp=BeautifulSoup(content)\n",
    "    try:\n",
    "        tmp_sp=sp.find('span',class_=\"author_name\").find('a').get_text()\n",
    "        return tmp_sp\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_-nadqGlMSq"
   },
   "outputs": [],
   "source": [
    "def tfidf_mean(content):\n",
    "    x=tf_transformer.transform(cv.transform([content]))\n",
    "    x_arr=np.array(x.toarray()[0])\n",
    "    return x_arr.sum()/np.count_nonzero(x_arr)\n",
    "def tf_mean_df(input_df):\n",
    "    input_df['tf_mean']=input_df['Page content_text'].apply(tfidf_mean)\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0gcB_i8lMSq"
   },
   "outputs": [],
   "source": [
    "def tfidf_count(content):\n",
    "    x=tf_transformer.transform(cv.transform([content]))\n",
    "    x_arr=x.toarray()[0]\n",
    "    sum_=0\n",
    "    for i in x_arr:\n",
    "        if(i!=0):\n",
    "            sum_=sum_+1\n",
    "    return sum_\n",
    "def tf_count_df(input_df):\n",
    "    input_df['tf_mean']=input_df['Page content_text'].apply(tfidf_count)\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYliMKPjlMSr"
   },
   "outputs": [],
   "source": [
    "def get_gatgory(row):\n",
    "    p=re.compile(\"<a href=\\\"/category/([a-z0-9]+)/\\\">\")    \n",
    "    return p.findall(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "f7fxu6xmlMSr",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "0cd24bdb-71d0-40b9-e414-43459d59d744"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world\n",
      "tech\n",
      "entertainment\n",
      "watercooler\n",
      "business\n",
      "us\n",
      "video\n",
      "mobile\n",
      "gadgets\n",
      "television\n",
      "photography\n",
      "lifestyle\n",
      "sports\n",
      "film\n",
      "music\n",
      "twitter\n",
      "travel\n",
      "startups\n",
      "marketing\n",
      "politics\n",
      "apple\n",
      "facebook\n",
      "media\n",
      "advertising\n",
      "gaming\n",
      "conversations\n",
      "google\n",
      "apps\n",
      "youtube\n",
      "space\n",
      "jobs\n",
      "humor\n",
      "home\n",
      "climate\n",
      "iphone\n",
      "android\n",
      "microsoft\n",
      "comics\n",
      "instagram\n",
      "samsung\n",
      "amazon\n",
      "gallery\n",
      "smartphone\n",
      "art\n",
      "australia\n",
      "ukraine\n",
      "memes\n",
      "celebrities\n",
      "health\n",
      "kickstarter\n"
     ]
    }
   ],
   "source": [
    "def one_hot_cat(row,key):\n",
    "    for i in row:\n",
    "        if(i==key):\n",
    "            return 1\n",
    "    return 0\n",
    "for i in range(50):\n",
    "    print(sorted_counter_object[i][0])\n",
    "    key=sorted_counter_object[i][0]\n",
    "    pre_df[key+'_cat']=pre_df['catgory'].apply(one_hot_cat,key=key)\n",
    "    pre_df[key+'_cat']=pre_df['catgory'].apply(one_hot_cat,key=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZVX1OX7lMSr"
   },
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBW296aPlMSs",
    "outputId": "e666ee1b-7505-4677-e486-8696c9591594"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=5, random_state=42)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA_count_df = CountVectorizer(ngram_range=(1, 1),\n",
    "                        tokenizer=tokenizer_stem_nostop, stop_words = 'english')\n",
    "LDA = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "LDA_count_df.fit(pre_df['Page content_text'])\n",
    "dtm = LDA_count_df.transform(pre_df['Page content_text'])\n",
    "LDA.fit(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFTlQYLxsS-f"
   },
   "source": [
    "### Feature Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FK6F216E2wN7",
    "outputId": "d35279ac-8b37-4ac9-ee83-9f8d51e7c9fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'Popularity', 'Page content_text', 'Weekend', 'img', 'title',\n",
      "       'link', 'Page content', 'figure', 'Fri', 'pop ch', 'world', 'socme',\n",
      "       'app', 'marketing', 'author', 'time', 'Sat', 'channel', 'youtube vedio',\n",
      "       'tech', 'watercooler', 'entertainment', 'startups', 'social-media',\n",
      "       'tf_mean', 'year', 'month', 'date', 'hour', 'month_en', 'catgory',\n",
      "       'world_cat', 'tech_cat', 'entertainment_cat', 'watercooler_cat',\n",
      "       'business_cat', 'us_cat', 'video_cat', 'mobile_cat', 'gadgets_cat',\n",
      "       'television_cat', 'photography_cat', 'lifestyle_cat', 'sports_cat',\n",
      "       'film_cat', 'music_cat', 'twitter_cat', 'travel_cat', 'startups_cat',\n",
      "       'marketing_cat', 'politics_cat', 'apple_cat', 'facebook_cat',\n",
      "       'media_cat', 'advertising_cat', 'gaming_cat', 'conversations_cat',\n",
      "       'google_cat', 'apps_cat', 'youtube_cat', 'space_cat', 'jobs_cat',\n",
      "       'humor_cat', 'home_cat', 'climate_cat', 'iphone_cat', 'android_cat',\n",
      "       'microsoft_cat', 'comics_cat', 'instagram_cat', 'samsung_cat',\n",
      "       'amazon_cat', 'gallery_cat', 'smartphone_cat', 'art_cat',\n",
      "       'australia_cat', 'ukraine_cat', 'memes_cat', 'celebrities_cat',\n",
      "       'health_cat', 'kickstarter_cat'],\n",
      "      dtype='object')\n",
      "Id                                                                   1\n",
      "Popularity                                                           1\n",
      "Page content_text     google took a stand of sorts against thursday...\n",
      "Weekend                                                              0\n",
      "img                                                                  2\n",
      "                                           ...                        \n",
      "ukraine_cat                                                          0\n",
      "memes_cat                                                            0\n",
      "celebrities_cat                                                      0\n",
      "health_cat                                                           0\n",
      "kickstarter_cat                                                      0\n",
      "Name: 1, Length: 82, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/content/drive/MyDrive/大四上資料/Deep_learning /COMP1/data v4/preprocess_df.csv') # input dataframe\n",
    "print(df.columns)\n",
    "print(df.loc[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0_2QtGWMXNO"
   },
   "source": [
    "# Saving Vectorized Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4wzFu-cYMc2U",
    "outputId": "e2853ea3-8182-451d-993d-efba4fa6fb47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11847, 100)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "df_raw = pd.read_csv('/content/drive/MyDrive/大四上資料/Deep_learning /COMP1/dataset_comp/train.csv')\n",
    "df = pd.read_csv('/content/drive/MyDrive/大四上資料/Deep_learning /COMP1/誠晉/df_new.csv') # input dataframe\n",
    "df_test = pd.read_csv('/content/drive/MyDrive/大四上資料/Deep_learning /COMP1/誠晉/modtest.csv')\n",
    "tfidf = pickle.load(open(\"/content/drive/MyDrive/大四上資料/Deep_learning /COMP1/vectorizer.pickle\", \"rb\")) # input vetorizer\n",
    "X = df['Page content']\n",
    "X_test = df_test['Page content']\n",
    "#print(df[\"iframe\"].idxmax())\n",
    "#print(df_raw[\"Page content\"].values[df[\"iframe\"].idxmax()])\n",
    "\n",
    "#X = tfidf.transform(X)\n",
    "#pickle.dump(X, open(\"/content/drive/MyDrive/大四上資料/Deep_learning /COMP1/X_train_content.pkl\", \"wb\"))\n",
    "X = tfidf.transform(X_test)\n",
    "pickle.dump(X, open(\"/content/drive/MyDrive/大四上資料/Deep_learning /COMP1/X_test_content.pkl\", \"wb\"))\n",
    "X = hstack([X, weekend.values.reshape(-1, 1), img.values.reshape(-1, 1), links.values.reshape(-1,1), videos.values.reshape(-1,1), df[\"Fri\"].values.reshape(-1,1)])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XcofNXJ04ATw",
    "outputId": "5cecf782-93f6-4307-d69f-772ed1ef11ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(df['Popularity'].values[df[\"iframe\"].idxmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RSlnAXLlMSu"
   },
   "source": [
    "# Model Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ow-m5Qz4rwxw"
   },
   "source": [
    "#SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95dd1cdb"
   },
   "outputs": [],
   "source": [
    "def get_stream(path, size):\n",
    "    for chunk in pd.read_csv(path, chunksize=size):\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "7bb4d37a",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "c9e62cb4-1288-4a23-ebda-c0d3f70a3d7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000/25000] 0.48958783686410146\n",
      "[4000/25000] 0.5382308965771573\n",
      "[6000/25000] 0.516750067000268\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-558f386ef010>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweekend2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Page content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Popularity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Weekend'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweekend2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   2099\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"The TF-IDF vectorizer is not fitted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2102\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1201\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-88cd98c6e930>\u001b[0m in \u001b[0;36mpreprocessor\u001b[0;34m(o_text)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# remove HTML tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mParserRejectedMarkup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m_feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0;31m# Close out any unfinished strings and close all the open tags.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bs4/builder/_htmlparser.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m             \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTMLParseError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/html/parser.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrawdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrawdata\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoahead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/html/parser.py\u001b[0m in \u001b[0;36mgoahead\u001b[0;34m(self, end)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstarttagopen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# < + letter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_starttag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"</\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_endtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/html/parser.py\u001b[0m in \u001b[0;36mparse_starttag\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_starttag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__starttag_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0mendpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_for_whole_start_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mendpos\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mendpos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/html/parser.py\u001b[0m in \u001b[0;36mcheck_for_whole_start_tag\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_for_whole_start_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mrawdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrawdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocatestarttagend_tolerant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Use SGDClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "# loss='log' gives logistic regression\n",
    "clf = SGDClassifier(loss='log', max_iter=100, tol=1e-3)\n",
    "batch_size = 1000\n",
    "stream = get_stream(path='/content/drive/MyDrive/大四上資料/Deep_learning /COMP1/Weekend&Img/df.csv', size=batch_size)\n",
    "classes = np.array([-1, 1])\n",
    "train_auc, val_auc = [], []\n",
    "# we use one batch for training and another for validation in each iteration\n",
    "iters = int((25000+batch_size-1)/(batch_size*2))\n",
    "for i in range(iters):\n",
    "    batch = next(stream)\n",
    "    X_train, y_train, weekend1, img1 = batch['Page content'], batch['Popularity'], batch['Weekend'], batch['Image']\n",
    "    if X_train is None:\n",
    "        break\n",
    "    X_train = tfidf.transform(X_train)\n",
    "    X_train = hstack([X_train, weekend1.values.reshape(-1, 1), img1.values.reshape(-1, 1)])\n",
    "    \n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    train_auc.append(roc_auc_score(y_train, clf.predict_proba(X_train)[:,1]))\n",
    "    \n",
    "    # validate\n",
    "    batch = next(stream)\n",
    "    X_val, y_val, weekend2, img2 = batch['Page content'], batch['Popularity'], batch['Weekend'], batch['Image']\n",
    "    X_val = tfidf.transform(X_val)\n",
    "    X_val = hstack([X_val, weekend2.values.reshape(-1, 1), img2.values.reshape(-1, 1)])\n",
    "    score = roc_auc_score(y_val, clf.predict_proba(X_val)[:,1])\n",
    "    val_auc.append(score)\n",
    "    print('[{}/{}] {}'.format((i+1)*(batch_size*2), 25000, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umX0gxZ_lM5z"
   },
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VaMt0p7RRvSs"
   },
   "source": [
    "* C=0.1, gamma=1, kernel=poly;, score=0.55625\n",
    "* C=0.1, gamma=0.1, kernel=poly; score=0.555\n",
    "* C=0.1, gamma=0.01, kernel=poly; score=0.555\n",
    "* C=1, gamma=1, kernel=poly; score=0.535\n",
    "* C=1, gamma=0.1, kernel=poly; score=0.5525\n",
    "* C=1, gamma=0.01, kernel=poly; score=0.555\n",
    "* C=10, gamma=1, kernel=poly, score = 0.512\n",
    "\n",
    "* C=0.1, gamma="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "stCYGzadP5Pr",
    "outputId": "02abd6d7-2ea7-4213-aa5f-8879fef14fed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 18 candidates, totalling 72 fits\n",
      "[CV 1/4] END .....C=0.1, gamma=1, kernel=linear;, score=0.557 total time= 3.7min\n",
      "[CV 2/4] END .....C=0.1, gamma=1, kernel=linear;, score=0.549 total time= 3.8min\n",
      "[CV 3/4] END .....C=0.1, gamma=1, kernel=linear;, score=0.541 total time= 3.7min\n",
      "[CV 4/4] END .....C=0.1, gamma=1, kernel=linear;, score=0.557 total time= 3.8min\n",
      "[CV 1/4] END ........C=0.1, gamma=1, kernel=rbf;, score=0.557 total time= 4.2min\n",
      "[CV 2/4] END ........C=0.1, gamma=1, kernel=rbf;, score=0.556 total time= 4.2min\n",
      "[CV 3/4] END ........C=0.1, gamma=1, kernel=rbf;, score=0.548 total time= 4.2min\n",
      "[CV 4/4] END ........C=0.1, gamma=1, kernel=rbf;, score=0.563 total time= 4.2min\n",
      "[CV 1/4] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.557 total time= 3.8min\n",
      "[CV 2/4] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.549 total time= 3.8min\n",
      "[CV 3/4] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.541 total time= 3.8min\n",
      "[CV 4/4] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.557 total time= 3.9min\n",
      "[CV 1/4] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.556 total time= 4.2min\n",
      "[CV 2/4] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.555 total time= 4.2min\n",
      "[CV 3/4] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.543 total time= 4.2min\n",
      "[CV 4/4] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.558 total time= 4.3min\n",
      "[CV 1/4] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.557 total time= 4.0min\n",
      "[CV 2/4] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.549 total time= 4.0min\n",
      "[CV 3/4] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.541 total time= 4.0min\n",
      "[CV 4/4] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.557 total time= 4.0min\n",
      "[CV 1/4] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.558 total time= 4.4min\n",
      "[CV 2/4] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.557 total time= 4.4min\n",
      "[CV 3/4] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.544 total time= 4.4min\n",
      "[CV 4/4] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.557 total time= 4.4min\n",
      "[CV 1/4] END .......C=1, gamma=1, kernel=linear;, score=0.557 total time= 4.1min\n",
      "[CV 2/4] END .......C=1, gamma=1, kernel=linear;, score=0.549 total time= 4.2min\n",
      "[CV 3/4] END .......C=1, gamma=1, kernel=linear;, score=0.540 total time= 4.1min\n",
      "[CV 4/4] END .......C=1, gamma=1, kernel=linear;, score=0.557 total time= 4.2min\n",
      "[CV 1/4] END ..........C=1, gamma=1, kernel=rbf;, score=0.554 total time= 4.6min\n",
      "[CV 2/4] END ..........C=1, gamma=1, kernel=rbf;, score=0.547 total time= 4.8min\n",
      "[CV 3/4] END ..........C=1, gamma=1, kernel=rbf;, score=0.551 total time= 5.0min\n",
      "[CV 4/4] END ..........C=1, gamma=1, kernel=rbf;, score=0.556 total time= 4.7min\n",
      "[CV 1/4] END .....C=1, gamma=0.1, kernel=linear;, score=0.557 total time= 4.3min\n",
      "[CV 2/4] END .....C=1, gamma=0.1, kernel=linear;, score=0.549 total time= 4.4min\n",
      "[CV 3/4] END .....C=1, gamma=0.1, kernel=linear;, score=0.540 total time= 4.2min\n",
      "[CV 4/4] END .....C=1, gamma=0.1, kernel=linear;, score=0.557 total time= 4.2min\n",
      "[CV 1/4] END ........C=1, gamma=0.1, kernel=rbf;, score=0.557 total time= 4.5min\n",
      "[CV 2/4] END ........C=1, gamma=0.1, kernel=rbf;, score=0.555 total time= 4.5min\n",
      "[CV 3/4] END ........C=1, gamma=0.1, kernel=rbf;, score=0.545 total time= 4.5min\n",
      "[CV 4/4] END ........C=1, gamma=0.1, kernel=rbf;, score=0.561 total time= 4.5min\n",
      "[CV 1/4] END ....C=1, gamma=0.01, kernel=linear;, score=0.557 total time= 4.2min\n",
      "[CV 2/4] END ....C=1, gamma=0.01, kernel=linear;, score=0.549 total time= 4.3min\n",
      "[CV 3/4] END ....C=1, gamma=0.01, kernel=linear;, score=0.540 total time= 4.2min\n",
      "[CV 4/4] END ....C=1, gamma=0.01, kernel=linear;, score=0.557 total time= 4.2min\n",
      "[CV 1/4] END .......C=1, gamma=0.01, kernel=rbf;, score=0.556 total time= 4.6min\n",
      "[CV 2/4] END .......C=1, gamma=0.01, kernel=rbf;, score=0.552 total time= 4.6min\n",
      "[CV 3/4] END .......C=1, gamma=0.01, kernel=rbf;, score=0.543 total time= 4.5min\n",
      "[CV 4/4] END .......C=1, gamma=0.01, kernel=rbf;, score=0.557 total time= 4.5min\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "\n",
    "X = pickle.load(open(\"/content/drive/MyDrive/大四上資料/Deep_learning /COMP1/X_train_content.pkl\", \"rb\"))\n",
    "df = pd.read_csv('/content/drive/MyDrive/大四上資料/Deep_learning /COMP1/誠晉/df_new.csv')\n",
    "y = df['Popularity']\n",
    "\n",
    "param_grid = {'C': [0.1,1, 10], 'gamma': [1,0.1,0.01],'kernel': ['linear', 'rbf']}\n",
    "\n",
    "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=4, scoring='roc_auc', cv=4)\n",
    "grid.fit(X,y)\n",
    "print(grid.best_estimator_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXLfq9KfYAB2"
   },
   "source": [
    "#Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "collapsed": true,
    "id": "DveWV-BHYCQ5",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "c9444082-9b2d-4ead-e772-950cac74f7e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 80 candidates, totalling 320 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-62f0963443ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    849\u001b[0m                     )\n\u001b[1;32m    850\u001b[0m                     for (cand_idx, parameters), (split_idx, (train, test)) in product(\n\u001b[0;32m--> 851\u001b[0;31m                         \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m                     )\n\u001b[1;32m    853\u001b[0m                 )\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    565\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    566\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X = pickle.load(open(\"/content/drive/MyDrive/大四上資料/Deep_learning /COMP1/X_train_content.pkl\", \"rb\"))\n",
    "df = pd.read_csv('/content/drive/MyDrive/大四上資料/Deep_learning /COMP1/Weekend&Img/df.csv')\n",
    "y = df['Popularity']\n",
    "abc = AdaBoostClassifier(base_estimator=DecisionTreeClassifier())\n",
    "\n",
    "parameters = {'base_estimator__max_depth':[i for i in range(2,11,2)],\n",
    "              'base_estimator__min_samples_leaf':[5,10],\n",
    "              'n_estimators':[10,50,250,1000],\n",
    "              'learning_rate':[0.01,0.1]}\n",
    "\n",
    "grid = GridSearchCV(abc, parameters ,verbose=4, scoring='roc_auc' ,n_jobs=-1, cv=4)\n",
    "grid.fit(X,y)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57axMc6XT0BY"
   },
   "source": [
    "#**Decision Trees** (Final Choice)\n",
    "##content, img, , weekend\n",
    "best: min_samples_leaf=6, n_estimators=20, random_state=11, criterion=entropy, max_dept=5\n",
    "##content, img, , weekend, link\n",
    "best: max_depth=4, n_estimators=30, random_state=11, criterion=entropy\n",
    "##content, img, weekend, link, videos, Fri\n",
    "{'criterion': 'gini', 'max_depth': 6, 'n_estimators': 800} auc = 0.5647\n",
    "##content, img, weekend, Fri\n",
    "{'criterion': 'gini', 'max_depth': 6, 'n_estimators': 800\n",
    "## all features expect LDAs\n",
    "{'criterion': 'gini', 'max_depth': 9, 'n_estimators': 1000} auc = 0.56775\n",
    "## all features\n",
    "{'criterion': 'gini', 'max_depth': 12, 'n_estimators': 1500}\n",
    "\n",
    "{'criterion': 'gini', 'max_depth': 11, 'n_estimators': 1500} auc = 0.568\n",
    "\n",
    "## all features + month + year + date\n",
    "{'criterion': 'gini', 'max_depth': 11, 'n_estimators': 1000} auc = 0.5795\n",
    "## all features + month + year + date + catagories + author + title\n",
    "{'criterion': 'gini', 'max_depth': 11, 'n_estimators': 1400 auc = 0.584"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "collapsed": true,
    "id": "tc0sLe04UdQv",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "d54693f8-d39c-4974-db12-f7fc53200327"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 381)\n",
      "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n",
      "[CV 1/4] END criterion=gini, max_depth=10, n_estimators=1000;, score=0.581 total time= 1.3min\n",
      "[CV 2/4] END criterion=gini, max_depth=10, n_estimators=1000;, score=0.568 total time= 1.3min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-ee61fb8ea112>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_noauthor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    849\u001b[0m                     )\n\u001b[1;32m    850\u001b[0m                     for (cand_idx, parameters), (split_idx, (train, test)) in product(\n\u001b[0;32m--> 851\u001b[0;31m                         \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m                     )\n\u001b[1;32m    853\u001b[0m                 )\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 289\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 289\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    465\u001b[0m                     \u001b[0mn_samples_bootstrap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_samples_bootstrap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m                 )\n\u001b[0;32m--> 467\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m             )\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 289\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 289\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    940\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m             \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m         )\n\u001b[1;32m    944\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    418\u001b[0m             )\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import pdb\n",
    "#quot_num = np.array(quot_num)\n",
    "df = pd.read_csv('/content/drive/MyDrive/大四上資料/Deep_learning /COMP1/誠晉/df_new.csv') # input dataframe\n",
    "\n",
    "df_2 = pd.read_csv('/content/drive/MyDrive/大四上資料/Deep_learning /COMP1/data v4/preprocess_df.csv')\n",
    "\n",
    "X = pickle.load(open(\"/content/drive/MyDrive/大四上資料/Deep_learning /COMP1/X_train_content.pkl\", \"rb\"))\n",
    "y, weekend, img, links, videos, sat = df['Popularity'], df['Weekend'], df['Img'], df[\"Link\"], df[\"Fri\"], df['Sat']\n",
    "title = tfidf.transform(df_2.loc[:, \"title\"]).toarray()\n",
    "XX = np.hstack([X.toarray(), df.loc[:,'World':'How-to'].values, df.loc[:,'Weekend':'Img'].values, \\\n",
    "               df.loc[:,\"0\":\"Video\"].values, df.loc[:,\"LDA1\":\"LDA5\"].values,\\\n",
    "               pd.get_dummies(df_2['month_en']).values, pd.get_dummies(df_2['year']).values,\\\n",
    "               pd.get_dummies(df_2['date']).values, pd.get_dummies(df_2['hour']).values, \\\n",
    "               title, pd.get_dummies(df_2[\"author\"]), df_2.loc[:,\"world_cat\": \"kickstarter_cat\"].values])\n",
    "X_noauthor = np.hstack([X.toarray(), df.loc[:,'World':'How-to'].values, df.loc[:,'Weekend':'Img'].values, \\\n",
    "               df.loc[:,\"0\":\"Video\"].values, df.loc[:,\"LDA1\":\"LDA5\"].values,\\\n",
    "               pd.get_dummies(df_2['month_en']).values, pd.get_dummies(df_2['year']).values,\\\n",
    "               pd.get_dummies(df_2['date']).values, pd.get_dummies(df_2['hour']).values, \\\n",
    "               title, df_2.loc[:,\"world_cat\": \"kickstarter_cat\"].values])\n",
    "#X2 = np.hstack([X.toarray(), df.iloc[:, ]]) \n",
    "print(X_noauthor.shape)\n",
    "\n",
    "\n",
    "y = df['Popularity']\n",
    "\n",
    "param_grid = {    \n",
    "    \"n_estimators\":[10,15,20],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"min_samples_leaf\": [2,4,6],\n",
    "    \"max_depth\": [2,3,4,'None']}\n",
    "param_grid2 = {\n",
    "    \"n_estimators\":[100], \n",
    "    \"criterion\": [\"entropy\", \"gini\"],\n",
    "    \"max_depth\": [3,4,5,6]\n",
    "}\n",
    "param_grid3 = {\n",
    "    \"n_estimators\":[1000, 1400], \n",
    "    \"criterion\": [\"gini\"],\n",
    "    \"max_depth\": [10, 11]\n",
    "}\n",
    "clf = RandomForestClassifier(random_state = 11)\n",
    "grid = GridSearchCV(clf, param_grid3,refit=True,verbose=5, scoring='roc_auc', cv=4)\n",
    "grid.fit(X_noauthor,y)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9uiYD0x4jN-"
   },
   "source": [
    "[CV 1/4] END criterion=gini, max_depth=6, n_estimators=800;, score=0.566 total time=  31.9s\n",
    "\n",
    "[CV 2/4] END criterion=gini, max_depth=6, n_estimators=800;, score=0.559 total time=  35.6s\n",
    "\n",
    "[CV 3/4] END criterion=gini, max_depth=6, n_estimators=800;, score=0.564 total time=  31.5s\n",
    "\n",
    "[CV 4/4] END criterion=gini, max_depth=6, n_estimators=800;, score=0.568 total time=  31.2s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVzG33wn9GPN"
   },
   "source": [
    "[CV 1/4] END criterion=gini, max_depth=8, n_estimators=1000;, score=0.567 total time=  50.2s\n",
    "\n",
    "[CV 2/4] END criterion=gini, max_depth=8, n_estimators=1000;, score=0.560 total time=  50.4s\n",
    "\n",
    "[CV 3/4] END criterion=gini, max_depth=8, n_estimators=1000;, score=0.567 total time=  50.4s\n",
    "\n",
    "[CV 4/4] END criterion=gini, max_depth=8, n_estimators=1000;, score=0.571 total time=  50.8s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bLyJPTQOfrn"
   },
   "source": [
    "[CV 1/4] END criterion=gini, max_depth=10, n_estimators=1000;, score=0.567 total time= 1.1min\n",
    "\n",
    "[CV 2/4] END criterion=gini, max_depth=10, n_estimators=1000;, score=0.560 total time= 1.2min\n",
    "\n",
    "[CV 3/4] END criterion=gini, max_depth=10, n_estimators=1000;, score=0.569 total time= 1.1min\n",
    "\n",
    "[CV 4/4] END criterion=gini, max_depth=10, n_estimators=1000;, score=0.576 total time= 1.2min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mHFORTdB8KLM",
    "outputId": "02ff0cdf-b577-4625-f6ad-ab5c6b924e00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_depth': 6, 'n_estimators': 30}\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzNHz5_Dsqys"
   },
   "source": [
    "## Our Final Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best result by 誠晉   \n",
    "Used 'Page content', 'Time', 'Weekend', 'Img', 'World',  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'Tech', 'Entertain', 'Business', 'Watercooler', 'Startups', 'Music',  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'Gadgets', 'Social-media', 'Apps', 'Marketing', 'Media', 'Gaming',  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'Mobile', 'Us', 'Pics', 'Dev', 'Small-busine', 'Film', 'How-to', 'Day',  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24',  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'Sun', 'Wed', 'Fri', 'Tue', 'Mon', 'Thu', 'Sat', 'Link', 'Video',  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'LDA1', 'LDA2', 'LDA3', 'LDA4', 'LDA5', 'Monthnum' as features.  \n",
    "Page content been tfidf transformed, and all other features duplicated twice.  \n",
    "Used random forrest as model, with n_estimator = 2000 and max_depth = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'Popularity', 'Page content', 'Time', 'Weekend', 'Img', 'World',\n",
      "       'Tech', 'Entertain', 'Business', 'Watercooler', 'Startups', 'Music',\n",
      "       'Gadgets', 'Social-media', 'Apps', 'Marketing', 'Media', 'Gaming',\n",
      "       'Mobile', 'Us', 'Pics', 'Dev', 'Small-busine', 'Film', 'How-to', 'Day',\n",
      "       '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n",
      "       '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24',\n",
      "       'Sun', 'Wed', 'Fri', 'Tue', 'Mon', 'Thu', 'Sat', 'Link', 'Video',\n",
      "       'LDA1', 'LDA2', 'LDA3', 'LDA4', 'LDA5', 'Monthnum'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('df.csv') # input dataframe\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ccc87\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "from pylab import *\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\"\"\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "import pickle\n",
    "# my functions\n",
    "import helpers.data_mining_helpers as dmh\n",
    "from sklearn.preprocessing import StandardScaler # scikit-learn 0.21.3\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "def preprocessor(o_text):\n",
    "    # remove content in time tag\n",
    "    text = BeautifulSoup(o_text)\n",
    "    time_tag = text.find(\"time\")\n",
    "    text = o_text.replace(str(time_tag),'')\n",
    "\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "     \n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    \n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "tfidf1 = TfidfVectorizer(max_features = 1000,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=preprocessor,\n",
    "                         tokenizer=tokenizer_stem_nostop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "717c6897",
    "outputId": "90685e82-1984-4358-fbbf-8446f3bfcfbe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=1000,\n",
       "                preprocessor=<function preprocessor at 0x000001A4018C1D30>,\n",
       "                tokenizer=<function tokenizer_stem_nostop at 0x000001A42DC9CCA0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf1.fit(df['Page content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7bd0a6d7"
   },
   "outputs": [],
   "source": [
    "X_train = tfidf1.transform(df['Page content'])\n",
    "X_train1 = hstack([X_train, df.loc[:,'Time':'Video'].values, df.loc[:,\"LDA1\":\"LDA5\"].values, df['Monthnum'].values.reshape(-1, 1)])\n",
    "X_train2 = hstack([X_train1, df.loc[:,'Time':'Video'].values, df.loc[:,\"LDA1\":\"LDA5\"].values, df['Monthnum'].values.reshape(-1, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use RandomForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(criterion='gini',max_depth=14,n_estimators=2000,n_jobs=-1)\n",
    "scores = cross_validate(estimator=clf, X=X_train2, y=df['Popularity'], cv=4, scoring='roc_auc',return_train_score=True)\n",
    "print('cross_tra_score: %.3f (+/-%.3f),  cross_val_score: %.3f (+/-%.3f)' % (scores['train_score'].mean(), scores['train_score'].std(), scores['test_score'].mean(), scores['test_score'].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgVaKCsKCSsT"
   },
   "source": [
    "## Prediction & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('modtest.csv')\n",
    "X_test = test['Page content']\n",
    "X_test = tfidf1.transform(X_test)\n",
    "X_test1 = hstack([X_test, test.loc[:,'Time':'Video'].values, test.loc[:,\"LDA1\":\"LDA5\"].values, test['Monthnum'].values.reshape(-1, 1)])\n",
    "X_test2 = hstack([X_test1, test.loc[:,'Time':'Video'].values, test.loc[:,\"LDA1\":\"LDA5\"].values, test['Monthnum'].values.reshape(-1, 1)])\n",
    "pred = clf.predict_proba(X_test2)\n",
    "pred1 = [x[1] for x in pred]\n",
    "submission = pd.DataFrame()\n",
    "submission['Id'] = test['Id']\n",
    "submission['Popularity'] = pred1\n",
    "print(submission)\n",
    "submission.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pe6RcgVylMSz"
   },
   "source": [
    "### Repeat features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcDMz0KwlMSz"
   },
   "source": [
    "The reason we hstack twice time in both training and testing data is because we find a fun fact that adding all features double times would benefit the training.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
